model_list:
  - model_name: zephyr # sample hosted open source model with openai compatible api
    litellm_params:
      model: openai/zephyr # the `openai/` prefix tells litellm it's openai compatible
      api_base: http://0.0.0.0:8000/v1/
      custom_llm_provider: openai
      api_key: "dummy"
  - model_name: mxbai-embed-large-v1-f16
    litellm_params:
      model: openai/mxbai-embed-large-v1-f16
      api_base: http://0.0.0.0:8000/v1/
      custom_llm_provider: openai
      api_key: "dummy"
litellm_settings:
  success_callback: ["langfuse"] # Optional callback when Langfuse is enabled in feature, and required env variables are set in the environment
  drop_params: True
  telemetry: False
  set_verbose: True
  cache: True          # set cache responses to True, litellm defaults to using a redis cache
  cache_params:
    type: "redis"
    ttl: 60
