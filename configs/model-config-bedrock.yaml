model_list:
  - model_name: meta.llama2-13b-chat-v1 ### RECEIVED MODEL NAME ###
    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
      model: bedrock/meta.llama2-13b-chat-v1 ### MODEL NAME sent to `litellm.completion()` ###
      aws_region_name: us-east-1
  - model_name: amazon.titan-embed-text-v1
    litellm_params:
      model: bedrock/amazon.titan-embed-text-v1
litellm_settings:
  callbacks : ["presidio"] # Optional callback when Presidio is enabled in feature
  success_callback: ["langfuse"] # Optional callback when Langfuse is enabled in feature, and required env variables are set in the environment
  drop_params: True
  telemetry: False
  set_verbose: True
  cache: True          # set cache responses to True, litellm defaults to using a redis cache
  cache_params:
    type: "redis"
    ttl: 60
