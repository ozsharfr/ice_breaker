{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: This is a sample notebook that gives hands on experience to some of the features of LLM Gateway. This notebook is meant to be interactive and might require the user to provide some inputs in designated code cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic usage\n",
    "This codespace was started with an open source LLM (phi3) with Ollama. The proxy service has been configured to use the same and the model is referenced in the code throughout this notebook by passing the 'model' parameter as 'phi3' to openai client completion calls. The proxy service runs on localhost port 4000 by default and has been referenced as the 'base_url' for openai client objects. The master key for LLM Gateway has been set as \"sk-password\" and can be used for calling registered models as well as accessing the admin panel at \"https://<CODESPACE_NAME>-4000.app.github.dev/ui\". The same can be accessed through port 4000 on the PORTS tab of the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"sk-password\",\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM\n",
    "Enter a prompt in the code cell below and run the next cell to call 'phi3' model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<your_prompt_here>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=\"phi3\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding \n",
    "This codespace is also equipped with an open source embedding model \"nomic-embed-text\" from Ollama. Try out the embedding model call from code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.embeddings.create(\n",
    "  model=\"nomic-embed-text\",\n",
    "  input=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching\n",
    "LLM Gateway provides a caching mechanism for LLM responses. This codespace has been configured to use Redis for cache. To see caching in action, write a prompt (more complex prompts, the better) in the code cell below and run the cells that follows and see the difference in response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"sk-password\",\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<your_prompt_here>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=\"phi3\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=\"phi3\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the traces logged to langfuse at \"https://<CODESPACE_NAME>-3000.app.github.dev\" or head over to the ports tab to access port 3000. It can be observed that when the response is returned from cache, the langfuse trace aptly marks \"True\" for cache hit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtual Keys\n",
    "The LLM Gateway in this codespace has been set up with a virtual key with access to the configured model for demonstration. Try out the code sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"sk-TE5BPNfSh4IOCNpW3I5EDQ\",\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<your_prompt_here>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=\"phi3\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PII Masking\n",
    "PII masking with presidio service is enabled in this codespace. Try out the code below to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<Enter_prompt_with_PII_here>\" # eg \"My name is John Doe and my social security number is 123-45-6789\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"sk-password\",\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(model=\"phi3\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the response have masked PII. It can also be seen in the langfuse traces corresponding to the LLM call that the gateway proxy service masks the data before passing the user inputs to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG with Langchain\n",
    "To demonstrate how models proxied from LLM Gateway maybe used in RAG applications with minimal code changes, a sample RAG pipeline is provided below. The RAG pipeline uses the open source model \"phi3\" registered through LLM Gateway to generate responses and the open source embedding model \"nomic-embed-text\" to generate embeddings for the documents. \n",
    "For this example, let's take the infamous state_of_the_union.txt as the document to be used in the RAG pipeline. Run the code below to see the RAG pipeline in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=\"sk-password\",\n",
    "    base_url=\"http://0.0.0.0:4000\",\n",
    "    model=\"nomic-embed-text\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"phi3\", temperature=0,api_key=\"sk-password\",\n",
    "    base_url=\"http://0.0.0.0:4000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load split documents into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Annoy.from_documents(\n",
    "    docs,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test RAG by asking a question about the state of the union address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<Enter_prompt_here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type = \"map_reduce\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.run(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
