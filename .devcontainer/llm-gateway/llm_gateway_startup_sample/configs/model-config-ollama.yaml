model_list:
  - model_name: phi3 # sample hosted open source model with openai compatible api
    litellm_params:
      model: ollama/phi3 
      api_base: http://0.0.0.0:11435
      api_key: "dummy"
  - model_name: nomic-embed-text # sample hosted open source model with openai compatible api
    litellm_params:
      model: ollama/nomic-embed-text 
      api_base: http://0.0.0.0:11435
      api_key: "dummy"
litellm_settings:
  callbacks : ["presidio"] # Optional callback when Presidio is enabled in feature
  success_callback: ["langfuse"] # Optional callback when Langfuse is enabled in feature, and required env variables are set in the environment
  drop_params: True
  telemetry: False
  set_verbose: True
  cache: True          # set cache responses to True, litellm defaults to using a redis cache
  cache_params:
    type: "redis-semantic"
    redis_semantic_cache_use_async: True
    similarity_threshold: 0.9   # similarity threshold for semantic cache
    redis_semantic_cache_embedding_model: nomic-embed-text  # set this to a model_name set in model_list
